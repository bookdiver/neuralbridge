{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralbridge.stochastic_processes.examples import OUProcess, OUAuxProcess\n",
    "from neuralbridge.networks.score_net import ScoreNetSmall\n",
    "from neuralbridge.stochastic_processes.conds import GuidedBridgeProcess, NeuralBridgeProcess\n",
    "from neuralbridge.solvers.sde import WienerProcess, Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:jax._src.xla_bridge:Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: dlopen(libtpu.so, 0x0001): tried: 'libtpu.so' (no such file), '/System/Volumes/Preboot/Cryptexes/OSlibtpu.so' (no such file), '/opt/homebrew/Caskroom/miniconda/base/envs/neuralbridge/bin/../lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache), 'libtpu.so' (no such file), '/usr/local/lib/libtpu.so' (no such file), '/usr/lib/libtpu.so' (no such file, not in dyld cache)\n"
     ]
    }
   ],
   "source": [
    "dim = 1\n",
    "T = 1.0\n",
    "dt = 1. / 200\n",
    "dtype = jnp.float32\n",
    "\n",
    "gamma = 1.0\n",
    "sigma = 1.0\n",
    "\n",
    "seed = 42\n",
    "u = jnp.array([0.0], dtype=dtype)\n",
    "v = jnp.array([0.0], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scheme = \"linear\"\n",
    "\n",
    "ou_proc = OUProcess(gamma, sigma, T, dim, dtype)\n",
    "ou_aux_proc = OUAuxProcess(gamma, sigma, T, dim, dtype)\n",
    "ou_guided_proc = GuidedBridgeProcess(\n",
    "    ou_proc, \n",
    "    ou_aux_proc,\n",
    "    u=u,\n",
    "    v=v,\n",
    "    L0=jnp.eye(dim, dtype=dtype),\n",
    "    Sigma0=jnp.eye(dim, dtype=dtype) * 1e-10,\n",
    "    ts=jnp.arange(0, T + dt, dt, dtype=dtype),\n",
    ")\n",
    "wiener_proc = WienerProcess(T, dt, dim, dtype, t_scheme)\n",
    "\n",
    "neural_net = ScoreNetSmall(\n",
    "    out_dim=dim,\n",
    "    hidden_dims=[10, 20],\n",
    "    activation=\"tanh\",\n",
    "    norm=\"layer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of mean w.r.t. gamma: -0.09112265706062317\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient w.r.t gamma using jax.grad\n",
    "grad_gamma = jax.grad(lambda g: jnp.mean(Euler(OUProcess(g, sigma, T, dim, dtype), wiener_proc).solve(x0=u, rng_key=jax.random.PRNGKey(42), batch_size=16).xs))(gamma)\n",
    "\n",
    "print(f\"Gradient of mean w.r.t. gamma: {grad_gamma}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient shapes:\n",
      "(10,)\n",
      "(2, 10)\n",
      "(20,)\n",
      "(10, 20)\n",
      "(1,)\n",
      "(20, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(20,)\n",
      "(20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': None, 'kernel': None},\n",
       " 'Dense_1': {'bias': None, 'kernel': None},\n",
       " 'Dense_2': {'bias': None, 'kernel': None},\n",
       " 'LayerNorm_0': {'bias': None, 'scale': None},\n",
       " 'LayerNorm_1': {'bias': None, 'scale': None}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = neural_net.init(jax.random.PRNGKey(42), jnp.zeros((1, 1)), jnp.zeros((1, dim))) \n",
    "neural_bridge = NeuralBridgeProcess(ou_guided_proc, neural_net)\n",
    "\n",
    "\n",
    "# Define a function that simulates trajectories and returns mean\n",
    "def simulate_and_mean(variables):\n",
    "    solver = Euler(neural_bridge, wiener_proc)\n",
    "    dWs = wiener_proc.sample_path(jax.random.PRNGKey(42), batch_size=16).dxs\n",
    "    trajectories = solver.solve_with_variables(\n",
    "        x0=u,\n",
    "        dWs=dWs,\n",
    "        variables=variables,\n",
    "        batch_size=16,\n",
    "        training=True,\n",
    "        mutable=[\"batch_stats\"]\n",
    "    )\n",
    "    return jnp.mean(trajectories.xs)\n",
    "\n",
    "# Compute gradients with respect to parameters only\n",
    "grad_params = jax.grad(lambda p: simulate_and_mean({'params': p, 'batch_stats': {}}))(variables['params'])\n",
    "\n",
    "# Print the shape of gradients for each parameter\n",
    "print(\"Gradient shapes:\")\n",
    "jax.tree.map(lambda x: print(x.shape), grad_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient shapes:\n",
      "(10,)\n",
      "(2, 10)\n",
      "(20,)\n",
      "(10, 20)\n",
      "(1,)\n",
      "(20, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(20,)\n",
      "(20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': None, 'kernel': None},\n",
       "  'Dense_1': {'bias': None, 'kernel': None},\n",
       "  'Dense_2': {'bias': None, 'kernel': None},\n",
       "  'LayerNorm_0': {'bias': None, 'scale': None},\n",
       "  'LayerNorm_1': {'bias': None, 'scale': None}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver = Euler(neural_bridge, wiener_proc)\n",
    "\n",
    "def loss_fn(variables, seed):\n",
    "    dWs = wiener_proc.sample_path(jax.random.PRNGKey(seed), batch_size=16).dxs\n",
    "    solver_paths = solver.solve_with_variables(\n",
    "        x0=u,\n",
    "        dWs=dWs,\n",
    "        variables=variables,\n",
    "        batch_size=16,\n",
    "        training=True   ,\n",
    "        mutable=[\"batch_stats\"]\n",
    "    )\n",
    "    xs, ts, log_lls = solver_paths.xs, solver_paths.ts, solver_paths.log_ll\n",
    "    ts = einops.repeat(ts, \"t -> b t 1\", b=xs.shape[0])\n",
    "    nus, *_ = neural_net.apply(\n",
    "        variables,\n",
    "        xs,\n",
    "        ts,\n",
    "        training=True,\n",
    "        mutable=[\"batch_stats\"]\n",
    "    )\n",
    "    loss = jnp.sum(jnp.sum(nus ** 2, axis=-1)) * dt - log_lls\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "# Compute gradients with respect to parameters\n",
    "grad_params_1 = jax.grad(loss_fn)(variables, seed=42)\n",
    "\n",
    "# Print the shape of gradients for each parameter\n",
    "print(\"Gradient shapes:\")\n",
    "jax.tree.map(lambda x: print(x.shape), grad_params_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient shapes:\n",
      "(10,)\n",
      "(2, 10)\n",
      "(20,)\n",
      "(10, 20)\n",
      "(1,)\n",
      "(20, 1)\n",
      "(10,)\n",
      "(10,)\n",
      "(20,)\n",
      "(20,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': None, 'kernel': None},\n",
       " 'Dense_1': {'bias': None, 'kernel': None},\n",
       " 'Dense_2': {'bias': None, 'kernel': None},\n",
       " 'LayerNorm_0': {'bias': None, 'scale': None},\n",
       " 'LayerNorm_1': {'bias': None, 'scale': None}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn_batch_input(variables, batch):\n",
    "    xs, ts, log_lls = batch\n",
    "    ts = einops.repeat(ts, \"t -> b t 1\", b=xs.shape[0])\n",
    "    nus, *_ = neural_net.apply(\n",
    "        variables={\n",
    "            \"params\": variables['params'],\n",
    "            \"batch_stats\": variables['batch_stats']\n",
    "        },\n",
    "        x=xs,\n",
    "        t=ts,\n",
    "        training=True,\n",
    "        mutable=[\"batch_stats\"]\n",
    "    )\n",
    "    loss = jnp.sum(jnp.sum(nus ** 2, axis=-1)) * dt - log_lls\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "path = solver.solve_with_variables(\n",
    "    x0=u, \n",
    "    dWs=wiener_proc.sample_path(jax.random.PRNGKey(42), batch_size=16).dxs, \n",
    "    variables=variables,\n",
    "    batch_size=16,\n",
    "    training=True,\n",
    "    mutable=[\"batch_stats\"]\n",
    ")\n",
    "batch = (path.xs, path.ts, path.log_ll)\n",
    "\n",
    "grad_params = jax.grad(lambda p: loss_fn_batch_input({'params': p, 'batch_stats': {}}, batch))(variables['params'])\n",
    "\n",
    "# Print the shape of gradients for each parameter\n",
    "print(\"Gradient shapes:\")\n",
    "jax.tree.map(lambda x: print(x.shape), grad_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralbridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
